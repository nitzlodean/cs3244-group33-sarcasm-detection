{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1746,"status":"ok","timestamp":1763799354783,"user":{"displayName":"zd","userId":"01712767252590713506"},"user_tz":-480},"id":"sGXWwsnon3rk","outputId":"5f5eee35-e0d4-4ac6-c721-de9cf7bc3483"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tuUVXt4wqx5H"},"outputs":[],"source":["import pandas as pd\n","import re\n","from sklearn.feature_extraction.text import TfidfVectorizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DxYd2KYZq_wQ"},"outputs":[],"source":["# load dataset\n","df = pd.read_csv(\n","    \"/content/drive/MyDrive/cs3244/dataset/train-balanced.csv.bz2\",\n","    compression=\"bz2\",\n","    sep=\"\\t\",\n","    header=None\n",")\n","\n","# assign column names for clarity\n","df.columns = [\n","    \"label\", \"text\", \"author\", \"subreddit\",\n","    \"upvotes\", \"downvotes\", \"score\",\n","    \"date\", \"timestamp\", \"comment\"\n","]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"baVwpc3Eq_ye"},"outputs":[],"source":["# text cleaning\n","def clean_text(text):\n","    if not isinstance(text, str):\n","        return \"\"\n","    return text.strip()\n","\n","df[\"text\"] = df[\"text\"].astype(str).apply(clean_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E-hoJDqjrD7O"},"outputs":[],"source":["# feature extraction functions\n","def count_uppercase_ratio(text):\n","    if len(text) == 0:\n","        return 0\n","    upper = sum(1 for c in text if c.isupper())\n","    return upper / len(text)\n","\n","def count_exclamations(text):\n","    return text.count(\"!\")\n","\n","def count_questions(text):\n","    return text.count(\"?\")\n","\n","def count_ellipsis(text):\n","    return text.count(\"...\")\n","\n","def count_elongated_words(text):\n","    # words with 3 or more repeated characters\n","    return len(re.findall(r\"(.)\\1{2,}\", text.lower()))\n","\n","def count_quoted_words(text):\n","    return len(re.findall(r\"\\\"[^\\\"]+\\\"|'[^']+'\", text))\n","\n","def has_hashtag_sarcasm(text):\n","    return int(bool(re.search(r\"#sarcasm|/s\", text.lower())))\n","\n","def avg_word_len(text):\n","    words = re.findall(r\"\\w+\", text)\n","    return sum(len(w) for w in words) / len(words) if words else 0"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19561,"status":"ok","timestamp":1763799406810,"user":{"displayName":"zd","userId":"01712767252590713506"},"user_tz":-480},"id":"Nt1StIuurD9E","outputId":"71d02064-9cfd-4d23-848d-37a4d1cbad55"},"outputs":[{"output_type":"stream","name":"stdout","text":["   label                                               text        author  \\\n","0      0                                   Dang dog, thanks    Mattys1174   \n","1      0  to summon the powers of the flying spaghetti m...    yugiohhero   \n","2      0       i did that 3rd last 1 by accident last night    yugiohhero   \n","3      0  He's insane, used him in DC, better than Blake...  masternater6   \n","4      0  Forgot about him, he's a pretty pointless card...  masternater6   \n","\n","         subreddit  upvotes  downvotes  score     date   timestamp  \\\n","0           NHLHUT        1         -1     -1  2017-03  1490927299   \n","1    CringeAnarchy        1         -1     -1  2017-02  1486349313   \n","2  TownofSalemgame        8         -1     -1  2017-03  1488514232   \n","3           NHLHUT        1         -1     -1  2017-02  1486097950   \n","4           NHLHUT        1         -1     -1  2017-04  1491597507   \n","\n","                                             comment  caps_ratio  excl_count  \\\n","0                             Great guess! Congrats!    0.062500           0   \n","1    can anynone explain what cash me outside means?    0.000000           0   \n","2                       I have seen all these things    0.000000           0   \n","3  ASG Weber review request Looking to pick him u...    0.080000           0   \n","4              Modano at least plays to his overall.    0.041667           0   \n","\n","   quest_count  ellipsis_count  elongation_count  quoted_words  \\\n","0            0               0                 0             0   \n","1            0               0                 0             0   \n","2            0               0                 0             0   \n","3            0               0                 0             0   \n","4            0               0                 0             0   \n","\n","   sarcasm_marker  avg_word_len  \n","0               0      4.333333  \n","1               0      4.625000  \n","2               0      3.500000  \n","3               0      3.454545  \n","4               0      3.800000  \n"]}],"source":["# apply features\n","df[\"caps_ratio\"] = df[\"text\"].apply(count_uppercase_ratio)\n","df[\"excl_count\"] = df[\"text\"].apply(count_exclamations)\n","df[\"quest_count\"] = df[\"text\"].apply(count_questions)\n","df[\"ellipsis_count\"] = df[\"text\"].apply(count_ellipsis)\n","df[\"elongation_count\"] = df[\"text\"].apply(count_elongated_words)\n","df[\"quoted_words\"] = df[\"text\"].apply(count_quoted_words)\n","df[\"sarcasm_marker\"] = df[\"text\"].apply(has_hashtag_sarcasm)\n","df[\"avg_word_len\"] = df[\"text\"].apply(avg_word_len)\n","\n","print(df.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cLDwfRN4rD_C","executionInfo":{"status":"ok","timestamp":1763799624811,"user_tz":-480,"elapsed":217993,"user":{"displayName":"zd","userId":"01712767252590713506"}},"outputId":"938ee86c-eb34-4281-c6fc-1d7aba67d01c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Word TF-IDF shape: (1125678, 10000)\n","Char TF-IDF shape: (1125678, 5000)\n"]}],"source":["# TF-IDF lexical features\n","# Word n-grams capture token sequences like “yeah right”\n","word_vectorizer = TfidfVectorizer(\n","    ngram_range=(1, 2),\n","    max_features=10000,\n","    lowercase=True,\n","    stop_words='english'\n",")\n","X_word_tfidf = word_vectorizer.fit_transform(df[\"text\"])\n","\n","# Character n-grams capture patterns like “!!!” or “soooo”\n","char_vectorizer = TfidfVectorizer(\n","    analyzer='char',\n","    ngram_range=(3, 5),\n","    max_features=5000\n",")\n","X_char_tfidf = char_vectorizer.fit_transform(df[\"text\"])\n","\n","print(\"Word TF-IDF shape:\", X_word_tfidf.shape)\n","print(\"Char TF-IDF shape:\", X_char_tfidf.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uUEm7tZRrSMu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763800288735,"user_tz":-480,"elapsed":663929,"user":{"displayName":"zd","userId":"01712767252590713506"}},"outputId":"680a2fa4-64e5-4ba1-b12b-ddcd3113adca"},"outputs":[{"output_type":"stream","name":"stdout","text":["Validation accuracy: 0.7144259469831569\n"]}],"source":["# combine engineered + TF-IDF features later for ML + logistic regression model\n","from scipy.sparse import hstack\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","\n","X_extra = df[[\"caps_ratio\",\"excl_count\",\"quest_count\",\"ellipsis_count\",\n","              \"elongation_count\",\"quoted_words\",\"sarcasm_marker\",\"avg_word_len\"]].values\n","X_full = hstack([X_word_tfidf, X_char_tfidf, X_extra])\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X_full, df[\"label\"], test_size=0.2, random_state=42\n",")\n","\n","model = LogisticRegression(max_iter=5000)\n","model.fit(X_train, y_train)\n","print(\"Validation accuracy:\", model.score(X_test, y_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ssxNwRT6HmM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763800308039,"user_tz":-480,"elapsed":19308,"user":{"displayName":"zd","userId":"01712767252590713506"}},"outputId":"c84b3a33-8f3f-48f5-8fe4-f7872f49fc8c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Saved feature dataframe to sarcasm_features.csv\n","Saved trained model to sarcasm_logreg_model.pkl\n","Saved vectorizers (word & char TF-IDF).\n"]}],"source":["import joblib\n","\n","# save df with engineered features\n","output_csv = \"sarcasm_features.csv\"\n","df.to_csv(output_csv, index=False)\n","print(f\"Saved feature dataframe to {output_csv}\")\n","\n","# save trained model\n","model_file = \"sarcasm_logreg_model.pkl\"\n","joblib.dump(model, model_file)\n","print(f\"Saved trained model to {model_file}\")\n","\n","# save TF-IDF vectorizers (for future inference)\n","joblib.dump(word_vectorizer, \"word_vectorizer.pkl\")\n","joblib.dump(char_vectorizer, \"char_vectorizer.pkl\")\n","print(\"Saved vectorizers (word & char TF-IDF).\")\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}